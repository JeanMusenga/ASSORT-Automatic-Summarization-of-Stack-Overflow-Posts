{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdkJOns8ISWbFrO7KMZYld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanMusenga/ASSORT-Automatic-Summarization-of-Stack-Overflow-Posts/blob/main/Boosting_StopwordRemoval_Lemmatization_InTextRank_V01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CLxCHdGaQ0Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVOxNU-ZQ3ww",
        "outputId": "7519a394-eb32-44c6-e492-a892cf69b204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5XZToiPPfuB",
        "outputId": "a74b29c1-540d-4154-9634-94f6534b4cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. I am still able to do\n",
            "stuff I need to do, I am just thinking, that tomorrow If I switch to a microservices approach where the user and student will be different microservices with different\n",
            "databases, I will not be able to inject the repository and somehow need to call the Rest API for user or student to achieve this. (score: 0.24370031296901695)\n",
            "2. Right now it's a Monolith and not a microservice architecture, but I am developing it in a way that tomorrow it will be easy to switch to Microservice. (score: 0.10526122466617574)\n",
            "3. I need help with the architecture pattern I should use in a NestJS project. (score: 0.0935469027596098)\n",
            "4. Considering this, creating users, contacts, and addresses are part of\n",
            "the user folder, and repository files and entity files related to these are stored in the user folder. (score: 0.07257372572430101)\n",
            "5. Create student, assign institute to the student, insert documents,\n",
            "are part of the student folder and repository, and entity files related to these are stored in the student folder. (score: 0.0681215254247017)\n"
          ]
        }
      ],
      "source": [
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text of the Stack Overflow question\n",
        "text = \"\"\"I need help with the architecture pattern I should use in a NestJS project. So I am using a command/query approach for developing my RestAPIs.\n",
        "Right now it's a Monolith and not a microservice architecture, but I am developing it in a way that tomorrow it will be easy to switch to Microservice.\n",
        "So consider a scenario, where I have 2 APIs one is createStudent and the other is createUser. In my application, I have 2 separate folders under src users and students\n",
        "where users will handle all stuff related to users and students will cater to student fees, attendance etc. Each of them has its own entities and repository files.\n",
        "Also, creating a student involves a step of creating a user as well. So basically let's say for a student to create, a user will be created, its contacts and address\n",
        "details will be saved, institute details will be saved, document details will be saved etc. Considering this, creating users, contacts, and addresses are part of\n",
        "the user folder, and repository files and entity files related to these are stored in the user folder. Create student, assign institute to the student, insert documents,\n",
        "are part of the student folder and repository, and entity files related to these are stored in the student folder. Right now what I am doing is, in createStudent handler,\n",
        "I have injected repositories for user, userAddresses and userContacts and using them in the handler to get/create/update records related to user, address or contacts.\n",
        "Though I have a separate handler for createUser as well, where I also need to do the same eventually, it will have nothing to do with the student. I am still able to do\n",
        "stuff I need to do, I am just thinking, that tomorrow If I switch to a microservices approach where the user and student will be different microservices with different\n",
        "databases, I will not be able to inject the repository and somehow need to call the Rest API for user or student to achieve this. Am I doing it the right way or Is there\n",
        "a way where I can call one handler from another handler in NestJS so that I can segregate the logic in their specific handlers? The second thought is, if users and students\n",
        "are so closely linked to each other and the exchange of data is happening, should those be segregated into different microservices or not?\"\"\"\n",
        "\n",
        "# Architectural keywords to focus on\n",
        "keywords = [\"architecture pattern\", \"architecture concern\", \"monolith\", \"microservice\", \"NestJS\", \"REST API\", \"system requirement\", \"repository\"]\n",
        "\n",
        "# Helper functions\n",
        "def normalize_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word.lower()) for word in words])\n",
        "\n",
        "# Preprocess: Split the text into sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# Create a version of sentences with stopwords removed for weighting purposes\n",
        "def remove_stopwords(sentence):\n",
        "    return ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n",
        "\n",
        "sentences_no_stopwords = [remove_stopwords(sentence) for sentence in sentences]\n",
        "\n",
        "# Build a TF-IDF vector representation of sentences without stopwords for weighting\n",
        "vectorizer = TfidfVectorizer().fit_transform(sentences_no_stopwords)\n",
        "vectors = vectorizer.toarray()\n",
        "\n",
        "# Compute cosine similarity matrix based on sentences without stopwords\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# Boost sentences containing keywords by increasing similarity scores\n",
        "def boost_similarity_for_keywords(sentences, similarity_matrix, keywords, boost_factor=1.5):\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        keyword_count = sum(keyword.lower() in sentence.lower() for keyword in keywords)\n",
        "        if keyword_count > 0:\n",
        "            # Boost similarity scores proportional to the number of keywords\n",
        "            similarity_matrix[i, :] *= boost_factor * keyword_count\n",
        "            similarity_matrix[:, i] *= boost_factor * keyword_count\n",
        "    return similarity_matrix\n",
        "\n",
        "# Apply boosting to the similarity matrix\n",
        "boosted_similarity_matrix = boost_similarity_for_keywords(sentences, similarity_matrix, keywords)\n",
        "\n",
        "# Build the similarity graph (nodes are sentences, edges are similarity scores)\n",
        "nx_graph = nx.from_numpy_array(boosted_similarity_matrix)\n",
        "\n",
        "# PageRank algorithm with the boosted similarity matrix\n",
        "scores = nx.pagerank(nx_graph)\n",
        "\n",
        "# Rank sentences by score (we use original sentences here with stopwords)\n",
        "ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "# Filter top sentences that also contain keywords\n",
        "def filter_top_sentences(ranked_sentences, keywords, top_n=5):\n",
        "    top_sentences = [sentence for _, sentence in ranked_sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
        "    return top_sentences[:top_n]\n",
        "\n",
        "# Get top 5 filtered sentences\n",
        "top_sentences = filter_top_sentences(ranked_sentences, keywords)\n",
        "\n",
        "# Output the final extractive summary with scores\n",
        "def get_summary(ranked_sentences, top_n=5):\n",
        "    return [(sentence, score) for score, sentence in ranked_sentences[:top_n]]\n",
        "\n",
        "# Display the summary with sentence scores\n",
        "summary = get_summary(ranked_sentences)\n",
        "for i, (sentence, score) in enumerate(summary, 1):\n",
        "    print(f\"{i}. {sentence} (score: {score})\")\n"
      ]
    }
  ]
}